Cleaning the Data
========================================================
Think about doing a hedonic price model.

I can tease out more information from the model by using the MIDAS model and get a better indication of 

This example compares various estimation techniques for forecasting GDP.  The techniques include ARIMA, a basic GARCH (1,1) model, time series regressions, and a MIDAS regression. The first 3 models were practiced at Johns Hopkins and the MIDAS model is built off from understanding these models and personal research.  MIDAS is a technique that allows models to use higher frequency data (in this case monthly) to explain less frequent data (in this case quarterly).This example covers cleaning and preparing the data sets and then analyising each model.  

Finally, each datset removes the last 4 periods of GDP data and then forecasts these periods to compare each model.

####Things to mention####
This is a quick analysis of an unruley dataset.  Each technique could be explored more in depth and many specifics about each model could change to increase the accuracy. 

The required librariess are: xlsx, zoo, fUnitRoots, tseries, rugarch, midasr, ggplot2, ggfortify,forecast.

```{r}
setwd("~/WorkExamples/MIDASModel") # taken care of by project directory
library(xlsx)
library(zoo)
library(fUnitRoots)
library(tseries)
library(rugarch)
library(midasr)
library(ggplot2)
library(ggfortify)
library(forecast)
```

###Cleaning and preping data set###
The data used in this example is from the federal reserve (FRED) website.  Rather than downloading remotely from the website database, it was downloaded directly. The data sets use are in the folder on github.

Employment Data: 'Total Nonfarm Payrolls, Thousands of Persons, Monthly, Seasonally Adjusted'

Real GDP Data: 'Real Gross Domestic Product, Billions of Dollars, Quarterly, Seasonally Adjusted Annual Rate'

There are a couple of issues with the data that should be discussed.  First, the data is created concurrently.  That means we are using data from the same period to predict data from the exact same period.  This is possible in this case, because GDP is released after employment data, so we can still use the employment information for the GDP calculation of that period.  Second, GDP data is revised 3 times and it is often heavily revised during these 3 periods.  The GDP data are based on the final estimate of GDP for the period.  This means that this analysis is geared towards the final estimate of GDP.  Even if it is correct, it may not look correct for several months after the initial GDP estimate.

The employment data starts in 1939, but the GDP data does not start until 1947.  Since GDP is the focus of this measurement, the first 8 years of data are dropped when importing the dataset.  

```{r}
employment<- read.xlsx('data/TotalNonfarmPayroll.xls', sheetIndex=1, sheetName=NULL, rowIndex=NULL,
                startRow=108, colIndex=2,
                as.data.frame=TRUE, header=FALSE)

gdp<- read.xlsx('data/GDP.xls', sheetIndex=1, sheetName=NULL, rowIndex=NULL,
                         startRow=20, colIndex=2,
                         as.data.frame=TRUE, header=TRUE)

```

The first step is to clean the data and to time-agregate the high frequency data.  GDP data is a quarterly measure of the economy while employment is a monthly number.  

```{r}
employment_quarters <- rollapply(employment[,1], width=3, mean, by=3)
employment_quarters <- data.frame(employment_quarters)

#Quick Test that it worked 
#employment_quarters[1] == mean(employment[1:3,1])
#employment_quarters[2] == mean(employment[4:6,1])

#Remove the final 4 values for Employment
employment_quarters <- data.frame(employment_quarters)
str(employment_quarters)
employment_test <- employment_quarters[(272:275) , ]
employment_training <- employment_quarters[ -(272:275) , ]

#Remore the final 4 values for GDP
gdp_quarters <- data.frame(gdp)
str(gdp) == str(employment_quarters)
gdp_test <- gdp[(272:275),]
gdp_training <- gdp[-(272:275),]

str(gdp_training) == str(employment_training)

#Set training data as time series
employment_q_ts<- ts(employment_training, start = c(1947, 1), frequency=4)
gdp_q_ts<- ts(gdp_training,start = c(1947, 1), frequency = 4)

#Set test data as time series and transform data
employment_test_form

```

###Initial Evaluation###
Looking at the plot for GDP, it is clear the data are not covairant stationary.
```{r fig.width=7, fig.height=6}
autoplot(gdp_q_ts, main = 'Total Real GDP', xlab = 'YEAR', ylab = 'GDP')
autoplot(log(gdp_q_ts), main = 'Log Real GDP', xlab = 'YEAR', ylab = 'GDP')

```

On this case, it makes sense to take the log of the data since GDP shows itself to generally be exponential. However, this isn't stationary either, so either a trend line or first differencing is necessary.  In this case, we jump to to first differncing with a line through the mean growth of gdp.  The graph should be stationary around this point.

```{r fig.width=7, fig.height=6}
autoplot(diff(log(gdp_q_ts)), main = 'Differenced Log Real GDP', xlab = 'YEAR', ylab = 'GDP') + geom_hline(yintercept = mean(diff(log(gdp_q_ts))), color = 'red')
```

The first differencing of the log looks stationary and the Augmented Dickey-Fuller Test confirms this instinct.  Here, it's compared to an ADF test of the log of GDP. In the case of the log of GDP, the null hypothesis can't be rejected, so the series needs to be differenced. When the logged series is differenced, the null can be rejected and the data are covariant stationary.

```{r}
adf.test(log(gdp_q_ts), alternative = c("stationary"), k = 16)
adf.test(diff(log(gdp_q_ts)), alternative = c("stationary"), k = 16)
```

###ARIMA Model###
The first step for ARIMA (p,d,q) models is to look at the Autocorrelation Function (acf) and Partial Autocorrelation Function (pacf) to identify the order of the model. When acf shows a seady decline in values it indicates any autocorrelation (AR) in the model.  When pacf shows a steady decline, it typically indicates any moving average (MA) in the model.  

It's important to note that while d term in our model shows no differencing, the data are differenced once once already, so if log(gdp) were used as the base data 'd' would equal 1.


```{r}
gdp_log_diff <-  diff(log(gdp_q_ts))

autoplot(acf(gdp_log_diff, lag= 16), ci = 0.95, main = "ACF")
autoplot(pacf(gdp_log_diff, lag= 16), ci = 0.95, main = "PACF")
```

The two charts above look back 16 periods, or 4 years.  For some reason, the zero lag is included in the acf plot but not the pacf, which starts at lag one. The acf cuts off at lag two and shows a cyclical decline in values, indicating an AR model. The pacf cuts off after the first lag, indicating an AR(1) for the model. However, I test multiple models. It's important to note that while the model does not indicate a lag, the data being used for the model has been differenced already.

```{r}
source('formulas/runArima.R')

Arima_table<- runArima(gdp_log_diff, ar=4, ma = 4, diff =0)
```

The ARIMA model (1,0,4) gives the lowest AIC of all the models tested, however, this does not make intuitive sense after looking at the afc and pafc.  Several models should be tested.

```{r}
ar1ma4 <- arima(gdp_log_diff, order= c(1,0,4))
summary(ar1ma4)

#Models I tried
#ma1<- arima(gdp_log_diff, order=c(0,0,1))
#ar1ma1<- arima(gdp_log_diff, order=c(1,0,1))
#ar2<- arima(gdp_log_diff,order=c(2,0,0))

#Model I selected
ar1<- arima(gdp_log_diff, order=c(1,0,0))
summary(ar1)
```

Looking at the ARIMA model (1,0,4), none of the MA terms show signifigance.  The model that I think works the best is the AR(1).  The ma1 shows signifigance at the 95% level and has a lower aic.  However, when an ARIMA (1,0,1) model is tries, the ma term does not show signifigance.  Also, the acf and pacf charts indicated that an AR(1) was the best model.


LOOK AT LOG LIKIHOOD. AIC AND SBC, WHAT IS THE DIFFERENCE?  TALK ABOUT WHY YOU LOOK AT ONE RATHER THAN THE OTHER. 

Talk about using the standard normal gaussian distribution.  We may want to use different distributions.
```{r}
tsdiag(ar1,gof=16)
Box.test(ar1$residuals,16,type='Ljung')
autoplot(acf(ar1$residuals))
autoplot(pacf(ar1$residuals))
autoplot(ar1$residuals)
```

The residuals, while passing the Ljung-Box statistic for every period in the previous 4 years, still look like the model shows heteroskedaticity. When looking at a plot of the residuals, it is easy to see there are two distict periods in the economy since GDP has been available, which could be creating the issues with our data.  The first period runs from 1947 to the early 1980s.  During this period, it's easy to see a lot more variance common in the variables.  There are several options.  The two most common would be to just ignore the beginning of the data.  This is commonly done with macro comparisons of the U.S. economy since the period since 1980 is seen as the 'modern' economy. 
 
###GARCH Model###
In this case however, we'd like to keep as much data as we can since there are only 4 instances of GDP every year.  To handle this conditional heterosedasticity, we can use a GARCH model to help contain the variability.  Most economic data is a GARCH (1,1).  For this example, we're assuming a GARCH (1,1) with a base ARIMA model of AR(1).

```{r}
autoplot(acf(ar1$residuals^2))
pacf((ar1$residuals^2))

garch_1 <- ugarchspec(mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),distribution.model = "norm")

ar1_garch<- ugarchfit(garch_1, data=gdp_log_diff)

autoplot(residuals(ar1_garch))
Box.test(residuals(ar1_garch),16,type='Ljung')
```
So there is still a lot of conditional heterosedasticity and does not look like it's much better than the AR model, but it is worth forcasting to see if it gives better predictions.

###Threshold GARCH Model###
```{r}
tgarch_1 <- ugarchspec(mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), variance.model = list(model = "fGARCH", garchOrder = c(1, 1), submodel = 'TGARCH'), distribution.model = "norm")

ar1_tgarch<- ugarchfit(tgarch_1, data=gdp_log_diff)

autoplot(residuals(ar1_tgarch))
Box.test(residuals(ar1_tgarch),16,type='Ljung')
```

###Adding employment as a regressor###
Since gdp was differenced and is not out for the 3rd Quarter at the time of the analysis (even though it is over), I need to strip out the first and last values to line up with the same number of values for GDP. (Make sure this makes logical sense).

```{r}
em_log_diff<- diff(log(employment_q_ts))
simple_lm <- lm(gdp_log_diff~em_log_diff)
summary(simple_lm)
```
A simple look at a linear model of employment's effect on changes in gdp shows a strong relationship.

```{r}
ar1_data <- arima(gdp_log_diff, order= c(1,0,0), xreg = em_log_diff)
summary(ar1_data)

tsdiag(ar1_data,gof=16)
Box.test(ar1_data$residuals,16,type='Ljung')
autoplot(ar1_data$residuals)
```

The AIC has gone p, but it is clear that employment is a factor in GDP.  The inclution of employment data has done more to improve the model than any of the GARCH models.  Even the residuals are more evenly distributed than before.  However, there is still a difference between the period before and after around 1980.  There were two recessions in the early 1980s, after which, the economy started to grow into the 1990s.  Just to see if we can get the model better, I set a dummy variable for before and after the beginning of 1983 (around the end of the second of these recessions) to see if there is any improvement in the model and to see if the dummy variable is signifigant in any way.

```{r}
#1983-1947 = 36
#36*4 = 144
#take off one period for the differencing so, the first 143 have a dummy value of 1, and the rest of the vector is 0.

#Setting up data
xregs <- data.frame(matrix(0,nrow=270,ncol=2))
xregs[,1] <- em_log_diff
xregs[1:143,2] <-1
colnames(xregs) <- c('Employment','Dummy')

#Formula
ar1_data_dummy <- arima(gdp_log_diff, order= c(1,0,0), xreg = xregs)
summary(ar1_data_dummy)


```

So the dummy variable is not signifigant and even if it were, the effect is very small.  As a result, we'll stick with just the employment data in the time series regression moving forward.

###MIDAS Models###
The midas model is a way of getting the most out of variables that have a higher frequency than the dependent variable.  Time series data, like the data here, is a good example where GDP is a quarterly data and employment is a monthly data set.  The most common way of dealing with the different frequencies is to average the frequent observations down to the least frequent variable.  This is what was done in the ARIMA regressions above where 3 months of  

 
```{r}
#Prep for MIDAS Regression
midas_employment <- diff(ts(employment))
#minus the first 2 for the first period out
midas_employment_base <- midas_employment[-(1:2),]

midas_employment_test <- data.frame(midas_employment_base[(811:822)])
midas_employment_training <- midas_employment_base[-(811:822)]
```

This is a quick look at the model without any ARIMA terms and all the stages of monthly employment are signifigant.

We have all the data for the 3rd quarter of gdp, but not the acutal gdp.

```{r}
#MIDSA model without the AR terms
midas_noar <- midas_r(gdp_log_diff ~  mls(midas_employment_training,0:2,3), start=NULL, data = list(gdp_log_diff = gdp_log_diff, midas_employment_training = midas_employment_training))
summary(midas_noar)
Box.test(midas_noar$residuals,16,type='Ljung')

autoplot(midas_noar$residuals)
#MIDAS with trend line
#trend <- 1:length(gdp_log_diff)
#midas_trend <- midas_r(gdp_log_diff ~ trend+ mls(midas_employment_diff,0:2,3), start=NULL, data = list(gdp_log_diff = gdp_log_diff, midas_employment_diff = midas_employment_diff))

#plot(midas_trend$residuals, type='l')

#AR(2) model
#midas_ar <- midas_r(gdp_log_diff ~ 
#                     mls(gdp_log_diff, 1, 1)+ mls(gdp_log_diff, 2, 1)  + mls(midas_employment_training,0:2,3), start=NULL)


midas_ar <- midas_r(gdp_log_diff ~ 
                      mls(gdp_log_diff, 1, 1)+ mls(midas_employment_training,0:2,3), start=NULL)

Box.test(midas_ar$residuals,16,type='Ljung')

acf(midas_ar$residuals)
pacf(midas_ar$residuals)
```

So the MIDAS AR(1) model is not perfect, but it would be intersting to take a look at this moving forward.  With the MIDAS model, it is also possible to look at lags greater than just the 3 from the current quarter.  In the example below, we look at the last 6 periods.

```{r}
# You can even look back further into the data than just 3 lags
midas_ar_6lags <- midas_r(gdp_log_diff ~ 
                      mls(gdp_log_diff, 1, 1)  + mls(midas_employment_training,0:5,3), start=NULL)

summary(midas_ar_6lags)
acf(midas_ar_6lags$residuals)
pacf(midas_ar_6lags$residuals)
Box.test(midas_ar_6lags$residuals,16,type='Ljung')
plot(midas_ar_6lags$residuals, type='l')
```

It is clear to see there is signifigance even before the most recent 3 months.  This makes intuitive sense as economic gain from new employment may only show it's full effect in the months after hiring rather than the same month.  

I look even futher back into the current quarter and the previous 2 quarters in the example below.

```{r}
midas_ar_9lags <- midas_r(gdp_log_diff ~ 
                              mls(gdp_log_diff, 1, 1)  + mls(midas_employment_training,0:8,3), start=NULL)

summary(midas_ar_8lags)
Box.test(midas_ar_8lags$residuals,16,type='Ljung')

```

Moving 9 lags back does not continue to produce positive results.  The AR term is no longer signifigant

###Forcasting Values for the given models###
Above are examples of an ARIMA modle, a TGARCH model, a Time Series Regression, and a MIDAS regression.  The best way to test these is on the last year's worth of data that we held out at the beginning. We can then directly compart the various models we have built.

```{r}
#ARIMA MODEL
ar1<- arima(gdp_log_diff, order=c(1,0,0))
ar1_perdict<- predict(ar1,n.ahead=4, newxreg = NULL, se.fit = TRUE)

#Time Series Regression
ar1_data <- arima(gdp_log_diff, order= c(1,0,0), xreg = em_log_diff)
ar1_data_predict <- predict(ar1_data, n.ahead=4, newxreg=employment_test, se.fit = TRUE)
#SOMETHING WRONG HERE
#NUMBERS TOO BIG, DON't MATCH AR(1) FORECAST

#MIDAS
midas_ar_6lags <- midas_r(gdp_log_diff ~ 
                      mls(gdp_log_diff, 1, 1)  + mls(midas_employment_training,0:5,3), start=NULL)

#STILL HAVING TROUBLE WITH THIS
midas_ar_6lags_predict <- forecast(midas_ar_6lags, newdata = midas_employment_test, se = TRUE, 
                      level =c(90, 95), method = "dynamic")


```
###Next Steps###
Many of the data sets 
          
        
        
        

```{r}
#Transform back to data frame, cut out last value and difference
#employment
employment_quarters<- rollapply(employment[,1], width=3, mean, by=3)
employment_quarters_diff <- diff(employment_quarters)
employment_quarters_diff <- data.frame(employment_quarters_diff)
employment_quarters_diff<- employment_quarters_diff[-274,]
employment_quarters_diff<- ts(employment_quarters_diff)
```

```{r}
# set up the data frames
gdp_log_diff_forecast <- data.frame(gdp_log_diff)
em_log_diff_forecast <- data.frame(em_log_diff)
midas_employment_forecast<- data.frame(midas_employment_diff)

#create data frames for analysis and comparison
gdp_actual <- gdp_log_diff_forecast[c(270:273),]
em_log_diff_actual <- em_log_diff_forecast[c(270:273),]
midas_employment_actual <- midas_employment_forecast[c(808:819),]
midas_employment_actual<- data.frame(midas_employment_actual)
colnames(midas_employment_actual)<- c('employment')


#take out the last 4 GDPs
gdp_log_diff_forecast <- gdp_log_diff_forecast[-c(270:273),]
gdp_log_diff_forecast <- ts(gdp_log_diff_forecast)

#take out last 4 Employments 
em_log_diff_forecast  <- em_log_diff_forecast [-c(270:273),]
em_log_diff_forecast  <- ts(em_log_diff_forecast)

#take 12 out of the midas
midas_employment_forecast <- midas_employment_forecast[-c(808:819),]
midas_employment_forecast <- data.frame(midas_employment_forecast)
```

```{r}
#take the handful of models you want to use and redo them
ar2_ma2_sa2_forecast <- arima(gdp_log_diff_forecast, order= c(2,0,2), seasonal = c(2,0,0))
ar2_ma2_predict <- predict(ar2_ma2_sa2_forecast, n.ahead= 4, newxreg = NULL)
#What is the predict static or dynamic?

#Arima with employment
ar_2_sa2_data_forecast <- arima(gdp_log_diff_forecast, order= c(2,0,2), seasonal = c(2,0,0), xreg = em_log_diff_forecast)
ar_2_sa2_data_predict <- predict(ar_2_sa2_data_forecast, n.ahead= 4, newxreg = em_log_diff_actual)

#Midas regression
#CANT FIGURE OUT HOW TO GET THE DATA TO BE GOOD
#midas_ar2_4lags_forecast <- midas_r(gdp_log_diff_forecast ~ 
#          mls(gdp_log_diff_forecast, 1, 1) + mls(midas_employment_forecast,0:3,3), start=NULL)

#midas_ar2_4lags_predict <- forecast(midas_ar2_4lags_forecast, 
#          list(midas_employment_forecast=  data.frame(midas_employment_actual$employment)), se = TRUE, level = c(90, 95), 
#          method = c("dynamic"))


#midas_ar2_4lags_predict <- forecast(midas_ar2_4lags_forecast, 
#          list(midas_employment_forecast= future), se = TRUE, level = c(90, 95), 
#          method = c("static"))

#nrow(midas_employment_actual$employment)

#insample= midas_employment_forecast

#simulate(midas_ar2_4lags_forecast, nsim = 4, , future = TRUE,
#newdata = matrix(midas_employment_actual$employment), insample = NULL, method = c("dynamic"),
#innov = NULL, show_progress = TRUE)
#test<- forecast(midas_ar_6lags, nsim = 4, future = FALSE)
# test<- simulate(midas_ar_6lags, nsim = 4, future = FALSE)

#midas_ar_6lags
#test<- simulate(midas_ar_6lags, nsim = 4, future = FALSE)
#CANT FIGURE OUT HOW TO GET THE DATA TO BE GOOD

#forecast(object, newdata = NULL, se = FALSE,
#level = c(80, 95), fan = FALSE, npaths = 999, method = c("static",
#"dynamic"), insample = get_estimation_sample(object),
#show_progress = TRUE, add_ts_info = FALSE, ...)

```